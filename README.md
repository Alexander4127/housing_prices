# Отчёт по Домашнему заданию 1, ML System Design


**Студент:** Куцаков Александр Сергеевич

**Кейс:** Предсказания стоимости дома по табличным признакам

**Тип данных:** табличные

**Бизнес-цель:** Регрессия

---

### **1. Данные**

**Источник данных:** Два набора данных
1. [Основной](https://www.kaggle.com/datasets/yasserh/housing-prices-dataset) небольшого размера и с большим количеством upvote
2. [Дополнительный](https://www.kaggle.com/datasets/sukhmandeepsinghbrar/housing-price-dataset) побольше, но с меньшей популярностью, использовался для weak supervision

**Лицензия:** ✔ Да, для любого типа использования у обоих наборов данных

**Weak Supervision:** Использовался двумя способами
1. Дополнительный набор данных (такая же целевая переменная и некоторые признаки) и дозаполнение признаков с обучением простейших моделей
2. Генерация синтетических данных (нормальное распределение для вещественных, категориальное для категориальных) и получение целевой переменной с помощью простейшей модели

**Объем данных:**

- **Train:** 422 + 21k (доп дата) + синтетика
- **Test:** 110

---

### **2. Метрики и разбиение данных**

**Основная метрика:** Relative MSE, 0.0443
$$
    RelMSE(y, \hat{y}) = \frac{1}{n} \sum_{i = 1}^n \frac{(\hat{y}_i - y_i)^2}{y_i^2} = MSE(\mathbb{1}, \hat{y} / y)
$$

**Дополнительные метрики:**
* RelMAE (по тому же принципу, что MSE) - не беру в качестве основной, потому что мне важно учитывать сильное отклонение, что хорошо делать с помощью MSE
* MSE, MAE - стандартные метрики, не использую основными, потому что сильнее реагируют на большие цены

**Способ разбиения:** Стратифицированное - я разбиваю выборку по квантилям целевой переменной и каждую подобласть уже разбиваю случайно.

---

### **3. Обработка данных**

✔ Дубликаты удалены: Да.

✔ Пропуски обработаны: Да, в основной выборке их не было, а в дополнительной я использовал регрессию по общим признакам, чтобы заполнить пропущенные колонки.

✔ Аугментации / Feature Engineering:
* Перевожу упорядоченные категориальные признаки, у которых есть редкие значения, в вещественные, чтобы не было проблем с выбросами и особенно с попаданием редких объектов только в валидационную выборку
* Использую логарифм площади, чтобы модель лучше могла учитывать её порядок (может помочь с выбросами)
* OneHot кодирование для категориальных, поскольку я проверяю, что редких классов почти нет

✔ Проверка на утечки: Внутри основного набора данных я удаляю слишком близкие пары (`price`, `area`) - вероятность совпадения очень мала и удаляется небольшое количество потенциально близких объектов. Далее я проверяю, что в дополнительном датасете никакая такая пара не совпадает с тестовой частью.

---

### **4. Бейзлайн**

**Выбранный бейзлайн:** Среднее по train таргету, Ridge. Первый, чтобы понимать, что модель что-то выучивает, второй достаточно простой и не сильно отличается от линейной регрессии, но при этом обладает регуляризацией и меньше переобучится.

**Качество бейзлайна:** 0.2140, 0.0443


### **5. Комментарии

Визуализация представлены в `exp.ipynb`, запуск эксперимента `run.py`.
